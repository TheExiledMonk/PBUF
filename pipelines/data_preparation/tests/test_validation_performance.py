"""
Validation and performance tests for the data preparation framework.

This module implements comprehensive validation and performance testing including:
- Round-trip tests verifying deterministic behavior with identical inputs
- Cross-validation tests comparing outputs with existing legacy loaders
- Performance tests ensuring full Phase A preparation â‰¤ 10 min on reference workstation

Requirements: 8.3, 9.1 - Validation and performance testing
"""

import pytest
import numpy as np
import time
import hashlib
import json
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Tuple
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timezone

from pipelines.data_preparation import DataPreparationFramework, StandardDataset
from pipelines.data_preparation.core.interfaces import ProcessingError
# Import derivation modules with fallback for testing
try:
    import pandas as pd
    if pd is None:
        raise ImportError("pandas not available")
    
    from pipelines.data_preparation.derivation.sn_derivation import SNDerivationModule
    from pipelines.data_preparation.derivation.bao_derivation import BAODerivationModule
    from pipelines.data_preparation.derivation.cmb_derivation import CMBDerivationModule
    from pipelines.data_preparation.derivation.cc_derivation import CCDerivationModule
    from pipelines.data_preparation.derivation.rsd_derivation import RSDDerivationModule
    DERIVATION_MODULES_AVAILABLE = True
except (ImportError, AttributeError) as e:
    print(f"Warning: Could not import derivation modules or pandas not available: {e}")
    DERIVATION_MODULES_AVAILABLE = False
from pipelines.dataset_registry.core.registry_manager import (
    RegistryManager, ProvenanceRecord, VerificationResult, EnvironmentInfo
)

# Import legacy loaders for cross-validation
try:
    from pipelines.fit_core.datasets import _load_dataset_legacy, load_dataset
    LEGACY_LOADERS_AVAILABLE = True
except ImportError:
    LEGACY_LOADERS_AVAILABLE = False


class SimpleTestDerivationModule:
    """Simple test derivation module that doesn't depend on external libraries."""
    
    def __init__(self, dataset_type: str = "test"):
        self._dataset_type = dataset_type
    
    @property
    def dataset_type(self) -> str:
        return self._dataset_type
    
    @property
    def supported_formats(self) -> List[str]:
        return ['.txt', '.csv', '.dat']
    
    def validate_input(self, raw_data_path: Path, metadata: Dict[str, Any]) -> bool:
        """Simple validation that just checks file exists and has content."""
        if not raw_data_path.exists():
            return False
        
        try:
            content = raw_data_path.read_text().strip()
            lines = [line for line in content.split('\n') if line.strip() and not line.startswith('#')]
            return len(lines) > 0
        except Exception:
            return False
    
    def derive(self, raw_data_path: Path, metadata: Dict[str, Any]) -> StandardDataset:
        """Simple derivation that parses comma-separated data."""
        content = raw_data_path.read_text().strip()
        lines = [line for line in content.split('\n') if line.strip() and not line.startswith('#')]
        
        z_values = []
        obs_values = []
        unc_values = []
        
        for line in lines:
            parts = line.split(',')
            if len(parts) >= 3:
                try:
                    z_values.append(float(parts[0]))
                    obs_values.append(float(parts[1]))
                    unc_values.append(float(parts[2]))
                except ValueError:
                    continue  # Skip invalid lines
        
        return StandardDataset(
            z=np.array(z_values),
            observable=np.array(obs_values),
            uncertainty=np.array(unc_values),
            covariance=None,
            metadata={
                **metadata,
                'source': metadata.get('source', 'test_source'),
                'processing_info': f'Processed by {self._dataset_type} derivation module',
                'n_points': len(z_values),
                'observable_type': f'{self._dataset_type}_observable'
            }
        )
    
    def get_transformation_summary(self) -> Dict[str, Any]:
        """Return transformation summary."""
        return {
            'transformation_steps': [
                'Parse comma-separated input file',
                'Extract z, observable, uncertainty columns',
                'Create StandardDataset'
            ],
            'formulas_used': ['No mathematical transformations applied'],
            'assumptions': ['Input data is already in correct units'],
            'references': [f'{self._dataset_type} test derivation module']
        }


class TestRoundTripDeterministic:
    """
    Round-trip tests that verify deterministic behavior with identical inputs.
    
    Tests that the framework produces identical outputs when processing the same
    input data multiple times, ensuring reproducible and deterministic behavior.
    
    Requirements: 8.3 - Round-trip tests for deterministic behavior
    """
    
    @pytest.fixture
    def mock_registry_manager(self):
        """Create mock registry manager for deterministic testing."""
        mock_manager = Mock(spec=RegistryManager)
        mock_manager.registry_path = Path("/tmp/test_registry")
        return mock_manager
    
    def _create_test_provenance(self, dataset_name: str, file_path: Path) -> ProvenanceRecord:
        """Create test provenance record."""
        environment = EnvironmentInfo(
            pbuf_commit="deterministic_test_commit",
            python_version="3.9.0",
            platform="Linux",
            hostname="test-host",
            timestamp=datetime.now(timezone.utc).isoformat()
        )
        
        verification = VerificationResult(
            sha256_verified=True,
            sha256_expected="deterministic_hash",
            sha256_actual="deterministic_hash",
            size_verified=True,
            size_expected=1000,
            size_actual=1000,
            schema_verified=True,
            schema_errors=[],
            verification_timestamp=datetime.now(timezone.utc).isoformat()
        )
        
        return ProvenanceRecord(
            dataset_name=dataset_name,
            download_timestamp=datetime.now(timezone.utc).isoformat(),
            source_used="deterministic_test_source",
            download_agent="test_agent",
            environment=environment,
            verification=verification,
            file_info={
                "local_path": str(file_path),
                "original_filename": f"{dataset_name}.txt",
                "mime_type": "text/plain"
            }
        )
    
    def _calculate_dataset_checksum(self, dataset: StandardDataset) -> str:
        """Calculate deterministic checksum for dataset."""
        # Create deterministic representation
        data_dict = {
            'z': dataset.z.tolist(),
            'observable': dataset.observable.tolist(),
            'uncertainty': dataset.uncertainty.tolist(),
            'covariance': dataset.covariance.tolist() if dataset.covariance is not None else None,
            'metadata_keys': sorted(dataset.metadata.keys())
        }
        
        # Convert to JSON string with sorted keys for determinism
        json_str = json.dumps(data_dict, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def test_sn_deterministic_processing(self, tmp_path, mock_registry_manager):
        """Test deterministic processing of supernova data."""
        # Create realistic SN test data
        sn_data = """# Supernova test data
# z, mu, sigma_mu
0.0233,32.81,0.12
0.0404,34.48,0.11
0.0593,35.58,0.13
0.0743,36.31,0.14
0.0930,37.03,0.15
"""
        test_file = tmp_path / "sn_deterministic.txt"
        test_file.write_text(sn_data)
        
        # Setup mock registry
        provenance = self._create_test_provenance("sn_deterministic", test_file)
        mock_registry_manager.get_registry_entry.return_value = provenance
        mock_registry_manager.has_registry_entry.return_value = True
        
        # Create framework
        framework = DataPreparationFramework(
            registry_manager=mock_registry_manager,
            output_directory=tmp_path / "output"
        )
        
        # Register SN module (use simple test module if real one not available)
        if DERIVATION_MODULES_AVAILABLE:
            sn_module = SNDerivationModule()
        else:
            sn_module = SimpleTestDerivationModule("sn")
        framework.register_derivation_module(sn_module)
        
        # Mock file operations
        with patch('pathlib.Path.exists', return_value=True), \
             patch('pathlib.Path.is_file', return_value=True), \
             patch('pathlib.Path.stat') as mock_stat, \
             patch.object(framework.registry_integration, '_calculate_file_checksum') as mock_checksum:
            
            mock_stat.return_value.st_size = len(sn_data)
            mock_checksum.return_value = "deterministic_hash"
            
            # Process dataset multiple times
            result1 = framework.prepare_dataset_from_registry("sn_deterministic")
            result2 = framework.prepare_dataset_from_registry("sn_deterministic")
            result3 = framework.prepare_dataset_from_registry("sn_deterministic")
        
        # Verify deterministic behavior
        assert np.array_equal(result1.z, result2.z)
        assert np.array_equal(result1.z, result3.z)
        assert np.array_equal(result1.observable, result2.observable)
        assert np.array_equal(result1.observable, result3.observable)
        assert np.array_equal(result1.uncertainty, result2.uncertainty)
        assert np.array_equal(result1.uncertainty, result3.uncertainty)
        
        # Verify checksums match (deterministic processing)
        checksum1 = self._calculate_dataset_checksum(result1)
        checksum2 = self._calculate_dataset_checksum(result2)
        checksum3 = self._calculate_dataset_checksum(result3)
        
        assert checksum1 == checksum2 == checksum3, "Deterministic processing failed - checksums differ"
        
        # Verify metadata consistency
        assert result1.metadata['n_points'] == result2.metadata['n_points'] == result3.metadata['n_points']
        assert result1.metadata['dataset_type'] == result2.metadata['dataset_type'] == result3.metadata['dataset_type']
    
    def test_bao_deterministic_processing(self, tmp_path, mock_registry_manager):
        """Test deterministic processing of BAO data."""
        # Create BAO test data
        bao_data = """# BAO test data
# z, DV_over_rd, sigma_DV_over_rd
0.15,4.47,0.17
0.32,8.88,0.17
0.57,13.77,0.13
"""
        test_file = tmp_path / "bao_deterministic.txt"
        test_file.write_text(bao_data)
        
        # Setup mock registry
        provenance = self._create_test_provenance("bao_deterministic", test_file)
        mock_registry_manager.get_registry_entry.return_value = provenance
        mock_registry_manager.has_registry_entry.return_value = True
        
        # Create framework
        framework = DataPreparationFramework(
            registry_manager=mock_registry_manager,
            output_directory=tmp_path / "output"
        )
        
        # Register BAO module (use simple test module if real one not available)
        if DERIVATION_MODULES_AVAILABLE:
            bao_module = BAODerivationModule()
        else:
            bao_module = SimpleTestDerivationModule("bao")
        framework.register_derivation_module(bao_module)
        
        # Mock file operations
        with patch('pathlib.Path.exists', return_value=True), \
             patch('pathlib.Path.is_file', return_value=True), \
             patch('pathlib.Path.is_dir', return_value=True), \
             patch('pathlib.Path.stat') as mock_stat, \
             patch.object(framework.registry_integration, '_calculate_file_checksum') as mock_checksum:
            
            mock_stat.return_value.st_size = len(bao_data)
            mock_checksum.return_value = "deterministic_hash"
            
            # Process dataset multiple times with different framework instances
            result1 = framework.prepare_dataset_from_registry("bao_deterministic")
            
            # Create new framework instance to test cross-instance determinism
            framework2 = DataPreparationFramework(
                registry_manager=mock_registry_manager,
                output_directory=tmp_path / "output2"
            )
            if DERIVATION_MODULES_AVAILABLE:
                bao_module2 = BAODerivationModule()
            else:
                bao_module2 = SimpleTestDerivationModule("bao")
            framework2.register_derivation_module(bao_module2)
            
            # Mock the second framework's registry integration as well
            with patch.object(framework2.registry_integration, '_calculate_file_checksum', return_value="deterministic_hash"):
                result2 = framework2.prepare_dataset_from_registry("bao_deterministic")
        
        # Verify deterministic behavior across framework instances
        assert np.array_equal(result1.z, result2.z)
        assert np.array_equal(result1.observable, result2.observable)
        assert np.array_equal(result1.uncertainty, result2.uncertainty)
        
        # Verify checksums match
        checksum1 = self._calculate_dataset_checksum(result1)
        checksum2 = self._calculate_dataset_checksum(result2)
        assert checksum1 == checksum2, "Cross-instance deterministic processing failed"
    
    def test_cmb_deterministic_processing(self, tmp_path, mock_registry_manager):
        """Test deterministic processing of CMB data."""
        # Create CMB test data with proper headers and uncertainties
        cmb_data = """R,l_A,theta_star,R_err,l_A_err,theta_star_err
1.7502,301.63,1.04119,0.0025,0.14,0.00030
"""
        test_file = tmp_path / "cmb_deterministic.txt"
        test_file.write_text(cmb_data)
        
        # Setup mock registry
        provenance = self._create_test_provenance("cmb_deterministic", test_file)
        mock_registry_manager.get_registry_entry.return_value = provenance
        mock_registry_manager.has_registry_entry.return_value = True
        
        # Create framework
        framework = DataPreparationFramework(
            registry_manager=mock_registry_manager,
            output_directory=tmp_path / "output"
        )
        
        # Register CMB module (use simple test module if real one not available)
        if DERIVATION_MODULES_AVAILABLE:
            cmb_module = CMBDerivationModule()
        else:
            cmb_module = SimpleTestDerivationModule("cmb")
        framework.register_derivation_module(cmb_module)
        
        # Mock file operations
        with patch('pathlib.Path.exists', return_value=True), \
             patch('pathlib.Path.is_file', return_value=True), \
             patch('pathlib.Path.is_dir', return_value=True), \
             patch('pathlib.Path.stat') as mock_stat, \
             patch.object(framework.registry_integration, '_calculate_file_checksum') as mock_checksum:
            
            mock_stat.return_value.st_size = len(cmb_data)
            mock_checksum.return_value = "deterministic_hash"
            
            # Process dataset multiple times
            results = []
            for i in range(5):  # Test multiple runs
                result = framework.prepare_dataset_from_registry("cmb_deterministic")
                results.append(result)
        
        # Verify all results are identical
        base_result = results[0]
        for i, result in enumerate(results[1:], 1):
            assert np.array_equal(base_result.z, result.z), f"Run {i+1} z values differ"
            assert np.array_equal(base_result.observable, result.observable), f"Run {i+1} observable values differ"
            assert np.array_equal(base_result.uncertainty, result.uncertainty), f"Run {i+1} uncertainty values differ"
            
            # Verify checksums
            base_checksum = self._calculate_dataset_checksum(base_result)
            result_checksum = self._calculate_dataset_checksum(result)
            assert base_checksum == result_checksum, f"Run {i+1} checksum differs"
    
    def test_environment_hash_consistency(self, tmp_path, mock_registry_manager):
        """Test that identical environment produces identical results."""
        # Create test data
        test_data = """0.1,1.0,0.1
0.2,2.0,0.2"""
        test_file = tmp_path / "env_test.txt"
        test_file.write_text(test_data)
        
        # Create identical environment info
        environment = EnvironmentInfo(
            pbuf_commit="fixed_commit_hash",
            python_version="3.9.0",
            platform="Linux",
            hostname="test-host",
            timestamp="2023-01-01T00:00:00Z"  # Fixed timestamp
        )
        
        # Create provenance with fixed environment
        verification = VerificationResult(
            sha256_verified=True,
            sha256_expected="env_test_hash",
            sha256_actual="env_test_hash",
            size_verified=True,
            size_expected=100,
            size_actual=100,
            schema_verified=True,
            schema_errors=[],
            verification_timestamp="2023-01-01T00:00:00Z"
        )
        
        provenance = ProvenanceRecord(
            dataset_name="env_test",
            download_timestamp="2023-01-01T00:00:00Z",
            source_used="env_test_source",
            download_agent="test_agent",
            environment=environment,
            verification=verification,
            file_info={
                "local_path": str(test_file),
                "original_filename": "env_test.txt",
                "mime_type": "text/plain"
            }
        )
        
        mock_registry_manager.get_registry_entry.return_value = provenance
        mock_registry_manager.has_registry_entry.return_value = True
        
        # Create framework with test module
        framework = DataPreparationFramework(
            registry_manager=mock_registry_manager,
            output_directory=tmp_path / "output"
        )
        
        # Register test module
        from pipelines.data_preparation.tests.test_integration import TestDerivationModule
        test_module = TestDerivationModule()
        framework.register_derivation_module(test_module)
        
        # Mock file operations
        with patch('pathlib.Path.exists', return_value=True), \
             patch('pathlib.Path.is_file', return_value=True), \
             patch('pathlib.Path.stat') as mock_stat, \
             patch.object(framework.registry_integration, '_calculate_file_checksum') as mock_checksum:
            
            mock_stat.return_value.st_size = 100
            mock_checksum.return_value = "env_test_hash"
            
            # Process with identical environment multiple times
            result1 = framework.prepare_dataset_from_registry("env_test")
            result2 = framework.prepare_dataset_from_registry("env_test")
        
        # Verify environment hash consistency
        env_hash1 = result1.metadata.get('provenance_summary', {}).get('environment_hash')
        env_hash2 = result2.metadata.get('provenance_summary', {}).get('environment_hash')
        
        assert env_hash1 == env_hash2, "Environment hash should be identical for identical environments"
        
        # Verify derived checksums match
        derived_checksum1 = result1.metadata.get('derived_checksum')
        derived_checksum2 = result2.metadata.get('derived_checksum')
        
        if derived_checksum1 and derived_checksum2:
            assert derived_checksum1 == derived_checksum2, "Derived checksums should match for identical inputs and environment"


@pytest.mark.skipif(not LEGACY_LOADERS_AVAILABLE, reason="Legacy loaders not available")
class TestCrossValidationWithLegacy:
    """
    Cross-validation tests comparing outputs with existing legacy loaders.
    
    Tests that verify the data preparation framework produces results that are
    compatible with and equivalent to existing legacy dataset loaders.
    
    Requirements: 8.3 - Cross-validation tests with legacy loaders
    """
    
    def test_sn_cross_validation(self, tmp_path):
        """Cross-validate SN processing with legacy loader."""
        # Skip if legacy loaders not available
        if not LEGACY_LOADERS_AVAILABLE:
            pytest.skip("Legacy loaders not available")
        
        # Create realistic SN test data that matches legacy format expectations
        sn_data = """# Supernova test data matching legacy format
# z, mu, sigma_mu
0.0233,32.81,0.12
0.0404,34.48,0.11
0.0593,35.58,0.13
0.0743,36.31,0.14
0.0930,37.03,0.15
"""
        test_file = tmp_path / "sn_cross_validation.txt"
        test_file.write_text(sn_data)
        
        # Create framework result
        framework = DataPreparationFramework()
        if DERIVATION_MODULES_AVAILABLE:
            sn_module = SNDerivationModule()
        else:
            sn_module = SimpleTestDerivationModule("sn")
        framework.register_derivation_module(sn_module)
        
        metadata = {
            'source': 'cross_validation_test',
            'version': '1.0',
            'description': 'Cross-validation test dataset'
        }
        
        framework_result = framework.prepare_dataset(
            "sn_cross_validation",
            raw_data_path=test_file,
            metadata=metadata
        )
        
        # Get legacy result (mock since we don't have real legacy data files)
        try:
            legacy_result = _load_dataset_legacy("sn")
            
            # Compare key properties
            # Note: We can't compare exact values since legacy uses different data,
            # but we can compare structure and data types
            
            # Convert framework result to legacy format for comparison
            from pipelines.data_preparation.output.format_converter import FormatConverter
            framework_as_dict = FormatConverter.standard_to_dataset_dict(framework_result, 'sn')
            
            # Verify structural compatibility
            assert 'observations' in framework_as_dict
            assert 'covariance' in framework_as_dict
            assert 'metadata' in framework_as_dict
            assert 'dataset_type' in framework_as_dict
            
            # Verify data types match
            framework_obs = framework_as_dict['observations']
            legacy_obs = legacy_result['observations']
            
            # Check that both have redshift data
            assert 'redshift' in framework_obs or 'z' in framework_obs
            assert 'redshift' in legacy_obs or 'z' in legacy_obs
            
            # Check that both have distance modulus data
            distance_keys = ['distance_modulus', 'mu', 'magnitude']
            framework_has_distance = any(key in framework_obs for key in distance_keys)
            legacy_has_distance = any(key in legacy_obs for key in distance_keys)
            
            assert framework_has_distance, "Framework result missing distance data"
            assert legacy_has_distance, "Legacy result missing distance data"
            
            print("âœ“ SN cross-validation passed - structural compatibility confirmed")
            
        except Exception as e:
            # If legacy loading fails, just verify framework result is valid
            assert isinstance(framework_result, StandardDataset)
            assert len(framework_result.z) == 5
            assert len(framework_result.observable) == 5
            assert len(framework_result.uncertainty) == 5
            print(f"âš ï¸  Legacy comparison skipped due to: {e}")
            print("âœ“ Framework SN processing validated independently")
    
    def test_bao_cross_validation(self, tmp_path):
        """Cross-validate BAO processing with legacy loader."""
        if not LEGACY_LOADERS_AVAILABLE:
            pytest.skip("Legacy loaders not available")
        
        # Create BAO test data
        bao_data = """# BAO test data
# z, DV_over_rd, sigma_DV_over_rd
0.15,4.47,0.17
0.32,8.88,0.17
0.57,13.77,0.13
"""
        test_file = tmp_path / "bao_cross_validation.txt"
        test_file.write_text(bao_data)
        
        # Create framework result
        framework = DataPreparationFramework()
        if DERIVATION_MODULES_AVAILABLE:
            bao_module = BAODerivationModule()
        else:
            bao_module = SimpleTestDerivationModule("bao")
        framework.register_derivation_module(bao_module)
        
        metadata = {
            'source': 'cross_validation_test',
            'version': '1.0',
            'description': 'BAO cross-validation test dataset'
        }
        
        framework_result = framework.prepare_dataset(
            "bao_cross_validation",
            raw_data_path=test_file,
            metadata=metadata
        )
        
        try:
            legacy_result = _load_dataset_legacy("bao")
            
            # Convert framework result for comparison
            from pipelines.data_preparation.output.format_converter import FormatConverter
            framework_as_dict = FormatConverter.standard_to_dataset_dict(framework_result, 'bao')
            
            # Verify structural compatibility
            assert framework_as_dict['dataset_type'] == 'bao'
            assert legacy_result['dataset_type'] == 'bao'
            
            # Check BAO-specific observables
            framework_obs = framework_as_dict['observations']
            legacy_obs = legacy_result['observations']
            
            # Both should have redshift and BAO distance measures
            bao_keys = ['DV_over_rd', 'DV_over_rs', 'dv_over_rd']
            framework_has_bao = any(key in framework_obs for key in bao_keys)
            legacy_has_bao = any(key in legacy_obs for key in bao_keys)
            
            assert framework_has_bao, "Framework result missing BAO distance measures"
            assert legacy_has_bao, "Legacy result missing BAO distance measures"
            
            print("âœ“ BAO cross-validation passed - structural compatibility confirmed")
            
        except Exception as e:
            # Validate framework result independently
            assert isinstance(framework_result, StandardDataset)
            assert len(framework_result.z) == 3
            assert framework_result.metadata['observable_type'] in ['DV_over_rd', 'bao_distance']
            print(f"âš ï¸  Legacy comparison skipped due to: {e}")
            print("âœ“ Framework BAO processing validated independently")
    
    def test_cmb_cross_validation(self, tmp_path):
        """Cross-validate CMB processing with legacy loader."""
        if not LEGACY_LOADERS_AVAILABLE:
            pytest.skip("Legacy loaders not available")
        
        # Create CMB test data
        cmb_data = """# CMB distance priors
# R, l_A, theta_star
1.7502,301.63,1.04119
"""
        test_file = tmp_path / "cmb_cross_validation.txt"
        test_file.write_text(cmb_data)
        
        # Create framework result
        framework = DataPreparationFramework()
        if DERIVATION_MODULES_AVAILABLE:
            cmb_module = CMBDerivationModule()
        else:
            cmb_module = SimpleTestDerivationModule("cmb")
        framework.register_derivation_module(cmb_module)
        
        metadata = {
            'source': 'cross_validation_test',
            'version': '1.0',
            'description': 'CMB cross-validation test dataset'
        }
        
        framework_result = framework.prepare_dataset(
            "cmb_cross_validation",
            raw_data_path=test_file,
            metadata=metadata
        )
        
        try:
            legacy_result = _load_dataset_legacy("cmb")
            
            # Convert framework result for comparison
            from pipelines.data_preparation.output.format_converter import FormatConverter
            framework_as_dict = FormatConverter.standard_to_dataset_dict(framework_result, 'cmb')
            
            # Verify CMB-specific structure
            assert framework_as_dict['dataset_type'] == 'cmb'
            assert legacy_result['dataset_type'] == 'cmb'
            
            # Check CMB observables
            framework_obs = framework_as_dict['observations']
            legacy_obs = legacy_result['observations']
            
            # Both should have CMB distance priors
            cmb_keys = ['R', 'l_A', 'theta_star', 'shift_parameter', 'acoustic_scale']
            framework_has_cmb = any(key in framework_obs for key in cmb_keys)
            legacy_has_cmb = any(key in legacy_obs for key in cmb_keys)
            
            assert framework_has_cmb, "Framework result missing CMB distance priors"
            assert legacy_has_cmb, "Legacy result missing CMB distance priors"
            
            print("âœ“ CMB cross-validation passed - structural compatibility confirmed")
            
        except Exception as e:
            # Validate framework result independently
            assert isinstance(framework_result, StandardDataset)
            assert framework_result.metadata['observable_type'] in ['cmb_distance_priors', 'cmb']
            print(f"âš ï¸  Legacy comparison skipped due to: {e}")
            print("âœ“ Framework CMB processing validated independently")
    
    def test_format_conversion_compatibility(self, tmp_path):
        """Test that framework outputs are compatible with legacy format expectations."""
        # Create test datasets for all types
        test_datasets = {
            'sn': """0.0233,32.81,0.12
0.0404,34.48,0.11""",
            'bao': """0.15,4.47,0.17
0.32,8.88,0.17""",
            'cmb': """1.7502,301.63,1.04119""",
            'cc': """0.17,83.8,3.0
0.27,77.0,14.0""",
            'rsd': """0.15,0.490,0.145
0.25,0.3512,0.0583"""
        }
        
        framework = DataPreparationFramework()
        
        # Register all modules
        if DERIVATION_MODULES_AVAILABLE:
            modules = {
                'sn': SNDerivationModule(),
                'bao': BAODerivationModule(),
                'cmb': CMBDerivationModule(),
                'cc': CCDerivationModule(),
                'rsd': RSDDerivationModule()
            }
        else:
            modules = {
                'sn': SimpleTestDerivationModule('sn'),
                'bao': SimpleTestDerivationModule('bao'),
                'cmb': SimpleTestDerivationModule('cmb'),
                'cc': SimpleTestDerivationModule('cc'),
                'rsd': SimpleTestDerivationModule('rsd')
            }
        
        for module in modules.values():
            framework.register_derivation_module(module)
        
        # Test each dataset type
        for dataset_type, data_content in test_datasets.items():
            test_file = tmp_path / f"{dataset_type}_compatibility.txt"
            test_file.write_text(data_content)
            
            metadata = {
                'source': f'{dataset_type}_compatibility_test',
                'version': '1.0'
            }
            
            # Process with framework
            framework_result = framework.prepare_dataset(
                f"{dataset_type}_compatibility",
                raw_data_path=test_file,
                metadata=metadata
            )
            
            # Convert to legacy format
            from pipelines.data_preparation.output.format_converter import FormatConverter
            legacy_format = FormatConverter.standard_to_dataset_dict(framework_result, dataset_type)
            
            # Verify legacy format structure
            required_keys = ['observations', 'covariance', 'metadata', 'dataset_type']
            for key in required_keys:
                assert key in legacy_format, f"Missing required key '{key}' in {dataset_type} legacy format"
            
            assert legacy_format['dataset_type'] == dataset_type
            assert isinstance(legacy_format['observations'], dict)
            assert isinstance(legacy_format['metadata'], dict)
            
            print(f"âœ“ {dataset_type.upper()} format conversion compatibility confirmed")


class TestPerformanceBenchmarks:
    """
    Performance tests ensuring full Phase A preparation â‰¤ 10 min on reference workstation.
    
    Tests that verify the framework meets performance requirements for processing
    all Phase A datasets within acceptable time limits.
    
    Requirements: 9.1 - Performance tests for Phase A dataset preparation
    """
    
    @pytest.fixture
    def performance_framework(self, tmp_path):
        """Create framework configured for performance testing."""
        framework = DataPreparationFramework(output_directory=tmp_path / "perf_output")
        
        # Register all derivation modules
        if DERIVATION_MODULES_AVAILABLE:
            modules = [
                SNDerivationModule(),
                BAODerivationModule(),
                CMBDerivationModule(),
                CCDerivationModule(),
                RSDDerivationModule()
            ]
        else:
            modules = [
                SimpleTestDerivationModule("sn"),
                SimpleTestDerivationModule("bao"),
                SimpleTestDerivationModule("cmb"),
                SimpleTestDerivationModule("cc"),
                SimpleTestDerivationModule("rsd")
            ]
        
        for module in modules:
            framework.register_derivation_module(module)
        
        return framework
    
    def _create_large_dataset(self, dataset_type: str, n_points: int) -> str:
        """Create large test dataset for performance testing."""
        if dataset_type == 'sn':
            # Create large SN dataset
            lines = ["# Large SN dataset for performance testing", "z,mb,dmb"]
            for i in range(n_points):
                z = 0.01 + i * 2.0 / n_points
                # Create realistic apparent magnitudes (mb) instead of distance modulus
                mb = 15.0 + 5 * np.log10(z * 3000 / 70)  # Realistic apparent magnitude
                sigma = 0.1 + 0.05 * np.random.random()
                lines.append(f"{z:.4f},{mb:.2f},{sigma:.3f}")
            return '\n'.join(lines)
        
        elif dataset_type == 'bao':
            # Create large BAO dataset
            lines = ["# Large BAO dataset for performance testing", "z,DV_over_rd,DV_over_rd_err"]
            for i in range(n_points):
                z = 0.1 + i * 1.5 / n_points
                dv_rd = 4 + 10 * z  # Approximate DV/rd scaling
                sigma = 0.1 + 0.05 * np.random.random()
                lines.append(f"{z:.3f},{dv_rd:.2f},{sigma:.3f}")
            return '\n'.join(lines)
        
        elif dataset_type == 'cmb':
            # CMB has fixed number of parameters
            return "# CMB dataset for performance testing\nz_star,theta_star,D_A\n1090.43,1.04119,301.63"
        
        elif dataset_type == 'cc':
            # Create large CC dataset
            lines = ["# Large CC dataset for performance testing", "z,H_z,H_z_err"]
            for i in range(n_points):
                z = 0.1 + i * 1.8 / n_points
                h_z = 70 * (0.3 * (1 + z)**3 + 0.7)**0.5  # Approximate H(z)
                sigma = 2 + 5 * np.random.random()
                lines.append(f"{z:.3f},{h_z:.1f},{sigma:.1f}")
            return '\n'.join(lines)
        
        elif dataset_type == 'rsd':
            # Create large RSD dataset
            lines = ["# Large RSD dataset for performance testing", "z,fsigma8,fsigma8_err"]
            for i in range(n_points):
                z = 0.1 + i * 1.2 / n_points
                fsig8 = 0.5 * (1 + z)**(-0.5)  # Approximate f*sigma8
                sigma = 0.02 + 0.03 * np.random.random()
                lines.append(f"{z:.3f},{fsig8:.4f},{sigma:.4f}")
            return '\n'.join(lines)
        
        else:
            raise ValueError(f"Unknown dataset type: {dataset_type}")
    
    def test_individual_dataset_performance(self, tmp_path, performance_framework):
        """Test performance of individual dataset processing."""
        dataset_configs = {
            'sn': {'n_points': 1000, 'max_time': 30.0},  # 30 seconds for 1000 SN
            'bao': {'n_points': 500, 'max_time': 20.0},   # 20 seconds for 500 BAO
            'cmb': {'n_points': 1, 'max_time': 5.0},      # 5 seconds for CMB
            'cc': {'n_points': 200, 'max_time': 15.0},    # 15 seconds for 200 CC
            'rsd': {'n_points': 100, 'max_time': 10.0}    # 10 seconds for 100 RSD
        }
        
        performance_results = {}
        
        for dataset_type, config in dataset_configs.items():
            # Create large test dataset
            data_content = self._create_large_dataset(dataset_type, config['n_points'])
            test_file = tmp_path / f"{dataset_type}_performance.txt"
            test_file.write_text(data_content)
            
            metadata = {
                'source': f'{dataset_type}_performance_test',
                'version': '1.0',
                'n_points': config['n_points']
            }
            
            # Measure processing time
            start_time = time.time()
            
            result = performance_framework.prepare_dataset(
                f"{dataset_type}_performance",
                raw_data_path=test_file,
                metadata=metadata
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Verify result
            assert isinstance(result, StandardDataset)
            if dataset_type not in ['cmb', 'cc']:  # CMB has fixed size, CC applies overlap filtering
                assert len(result.z) == config['n_points']
            elif dataset_type == 'cc':
                # CC applies overlap filtering, so expect fewer points than input
                assert len(result.z) <= config['n_points']
                assert len(result.z) > 0
            
            # Check performance requirement
            max_time = config['max_time']
            performance_results[dataset_type] = {
                'processing_time': processing_time,
                'max_allowed': max_time,
                'n_points': config['n_points'],
                'passed': processing_time <= max_time
            }
            
            print(f"âœ“ {dataset_type.upper()}: {processing_time:.2f}s / {max_time}s "
                  f"({config['n_points']} points) - {'PASS' if processing_time <= max_time else 'FAIL'}")
            
            # Assert performance requirement
            assert processing_time <= max_time, \
                f"{dataset_type} processing took {processing_time:.2f}s, exceeds limit of {max_time}s"
        
        # Print summary
        total_time = sum(r['processing_time'] for r in performance_results.values())
        total_allowed = sum(r['max_allowed'] for r in performance_results.values())
        
        print(f"\nðŸ“Š Individual Performance Summary:")
        print(f"   Total processing time: {total_time:.2f}s / {total_allowed}s")
        print(f"   All individual tests: {'PASS' if all(r['passed'] for r in performance_results.values()) else 'FAIL'}")
    
    def test_phase_a_full_pipeline_performance(self, tmp_path, performance_framework):
        """Test full Phase A dataset preparation pipeline performance (â‰¤ 10 min)."""
        # Phase A datasets: CMB, SN, BAO (isotropic and anisotropic)
        phase_a_datasets = {
            'cmb': self._create_large_dataset('cmb', 1),
            'sn': self._create_large_dataset('sn', 740),  # Pantheon+ size
            'bao_iso': self._create_large_dataset('bao', 15),  # Typical BAO compilation size
            'bao_ani': """z,dm_rd,dh_rd,dm_rd_err,dh_rd_err
0.38,10.23,13.36,0.17,0.21
0.51,13.36,11.79,0.18,0.19"""  # Anisotropic format with headers
        }
        
        # Maximum allowed time: 10 minutes = 600 seconds
        MAX_PHASE_A_TIME = 600.0
        
        print(f"\nðŸš€ Starting Phase A Full Pipeline Performance Test")
        print(f"   Target: Complete all Phase A datasets in â‰¤ {MAX_PHASE_A_TIME/60:.1f} minutes")
        
        start_time = time.time()
        results = {}
        
        for dataset_name, data_content in phase_a_datasets.items():
            dataset_start = time.time()
            
            # Create test file
            test_file = tmp_path / f"{dataset_name}_phase_a.txt"
            test_file.write_text(data_content)
            
            metadata = {
                'source': f'phase_a_{dataset_name}',
                'version': '1.0',
                'phase': 'A'
            }
            
            # Process dataset
            result = performance_framework.prepare_dataset(
                f"{dataset_name}_phase_a",
                raw_data_path=test_file,
                metadata=metadata
            )
            
            dataset_time = time.time() - dataset_start
            results[dataset_name] = {
                'result': result,
                'processing_time': dataset_time
            }
            
            print(f"   âœ“ {dataset_name}: {dataset_time:.2f}s")
        
        total_time = time.time() - start_time
        
        # Verify all results
        for dataset_name, data in results.items():
            assert isinstance(data['result'], StandardDataset)
            assert len(data['result'].z) > 0
            assert len(data['result'].observable) > 0
            assert len(data['result'].uncertainty) > 0
        
        # Check performance requirement
        print(f"\nðŸ“Š Phase A Pipeline Performance Results:")
        print(f"   Total processing time: {total_time:.2f}s ({total_time/60:.2f} minutes)")
        print(f"   Performance target: {MAX_PHASE_A_TIME}s ({MAX_PHASE_A_TIME/60:.1f} minutes)")
        print(f"   Result: {'âœ… PASS' if total_time <= MAX_PHASE_A_TIME else 'âŒ FAIL'}")
        
        # Detailed breakdown
        print(f"\n   Breakdown by dataset:")
        for dataset_name, data in results.items():
            pct = (data['processing_time'] / total_time) * 100
            print(f"     {dataset_name}: {data['processing_time']:.2f}s ({pct:.1f}%)")
        
        # Assert performance requirement
        assert total_time <= MAX_PHASE_A_TIME, \
            f"Phase A pipeline took {total_time:.2f}s ({total_time/60:.2f} min), " \
            f"exceeds 10-minute limit ({MAX_PHASE_A_TIME}s)"
    
    def test_memory_performance(self, tmp_path, performance_framework):
        """Test memory usage during large dataset processing."""
        import psutil
        import os
        
        # Create very large SN dataset to test memory efficiency
        large_sn_data = self._create_large_dataset('sn', 5000)  # 5000 supernovae
        test_file = tmp_path / "large_sn_memory_test.txt"
        test_file.write_text(large_sn_data)
        
        metadata = {
            'source': 'memory_performance_test',
            'version': '1.0',
            'n_points': 5000
        }
        
        # Get initial memory usage
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Process large dataset
        start_time = time.time()
        result = performance_framework.prepare_dataset(
            "large_sn_memory_test",
            raw_data_path=test_file,
            metadata={**metadata, 'dataset_type': 'sn'}
        )
        processing_time = time.time() - start_time
        
        # Get peak memory usage
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory
        
        # Verify result
        assert isinstance(result, StandardDataset)
        # Note: Some duplicates may be removed during processing
        assert len(result.z) > 1000  # Should have processed a significant amount
        
        # Memory usage should be reasonable (< 500 MB increase for 5000 points)
        MAX_MEMORY_INCREASE = 500  # MB
        
        print(f"\nðŸ’¾ Memory Performance Results:")
        print(f"   Dataset size: 5000 data points")
        print(f"   Processing time: {processing_time:.2f}s")
        print(f"   Initial memory: {initial_memory:.1f} MB")
        print(f"   Peak memory: {peak_memory:.1f} MB")
        print(f"   Memory increase: {memory_increase:.1f} MB")
        print(f"   Memory limit: {MAX_MEMORY_INCREASE} MB")
        print(f"   Result: {'âœ… PASS' if memory_increase <= MAX_MEMORY_INCREASE else 'âŒ FAIL'}")
        
        assert memory_increase <= MAX_MEMORY_INCREASE, \
            f"Memory usage increased by {memory_increase:.1f} MB, exceeds limit of {MAX_MEMORY_INCREASE} MB"
    
    def test_concurrent_processing_performance(self, tmp_path, performance_framework):
        """Test performance with concurrent dataset processing."""
        import threading
        import queue
        
        # Create multiple test datasets
        datasets = {}
        for i in range(3):  # Process 3 datasets concurrently
            dataset_name = f"sn_concurrent_{i}"  # Use sn prefix to ensure correct type detection
            data_content = self._create_large_dataset('sn', 200)  # Smaller datasets for concurrency
            test_file = tmp_path / f"{dataset_name}.txt"
            test_file.write_text(data_content)
            datasets[dataset_name] = test_file
        
        # Process datasets concurrently
        results_queue = queue.Queue()
        threads = []
        
        def process_dataset(name, file_path):
            try:
                start_time = time.time()
                result = performance_framework.prepare_dataset(
                    name,
                    raw_data_path=file_path,
                    metadata={'source': 'concurrent_test', 'version': '1.0', 'dataset_type': 'sn'}
                )
                processing_time = time.time() - start_time
                results_queue.put((name, result, processing_time, None))
            except Exception as e:
                results_queue.put((name, None, 0, e))
        
        # Start concurrent processing
        concurrent_start = time.time()
        
        for dataset_name, file_path in datasets.items():
            thread = threading.Thread(target=process_dataset, args=(dataset_name, file_path))
            threads.append(thread)
            thread.start()
        
        # Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        concurrent_total_time = time.time() - concurrent_start
        
        # Collect results
        concurrent_results = {}
        while not results_queue.empty():
            name, result, proc_time, error = results_queue.get()
            if error:
                pytest.fail(f"Concurrent processing failed for {name}: {error}")
            concurrent_results[name] = {'result': result, 'time': proc_time}
        
        # Verify all datasets processed successfully
        assert len(concurrent_results) == 3
        for name, data in concurrent_results.items():
            assert isinstance(data['result'], StandardDataset)
            assert len(data['result'].z) == 200
        
        # Compare with sequential processing time
        sequential_start = time.time()
        for dataset_name, file_path in datasets.items():
            performance_framework.prepare_dataset(
                f"{dataset_name}_sequential",
                raw_data_path=file_path,
                metadata={'source': 'sequential_test', 'version': '1.0'}
            )
        sequential_total_time = time.time() - sequential_start
        
        print(f"\nâš¡ Concurrent Processing Performance:")
        print(f"   Concurrent processing: {concurrent_total_time:.2f}s")
        print(f"   Sequential processing: {sequential_total_time:.2f}s")
        print(f"   Speedup factor: {sequential_total_time/concurrent_total_time:.2f}x")
        
        # Concurrent should be faster or at least not significantly slower
        # Allow up to 50% overhead for thread management
        max_allowed_concurrent = sequential_total_time * 1.5
        
        assert concurrent_total_time <= max_allowed_concurrent, \
            f"Concurrent processing too slow: {concurrent_total_time:.2f}s vs max {max_allowed_concurrent:.2f}s"


if __name__ == "__main__":
    # Run validation and performance tests
    pytest.main([__file__, "-v", "--tb=short"])